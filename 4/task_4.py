# -*- coding: utf-8 -*-
"""task 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18zU9AmRoJdsAL3_EgbgfY3OhviO1LMZE
"""

import os
os.environ["GROQ_API_KEY"] = ""
os.environ["SERPAPI_KEY"] = "S85fa10990995de2b0528181e3a85b6db11cee6a6e9b788dc54c0c19701783a34"


!pip install gradio requests


!pip install PyPDF2 scikit-learn gradio requests


import os
import requests
import gradio as gr
import json
import time
from PyPDF2 import PdfReader
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# =========================
# üîë API Keys
# =========================
GROQ_API_KEY = os.getenv("GROQ_API_KEY") or "YOUR_GROQ_KEY"
SERPAPI_KEY = os.getenv("SERPAPI_KEY") or "YOUR_SERPAPI_KEY"

# =========================
# üìÇ RAG Document Storage
# =========================
doc_chunks = []
vectorizer = None
embeddings = None

def process_document(file):
    """Process uploaded document into chunks + embeddings"""
    global doc_chunks, vectorizer, embeddings
    text = ""

    if file.name.endswith(".pdf"):
        reader = PdfReader(file.name)
        for page in reader.pages:
            text += page.extract_text() + " "
    else:
        text = file.read().decode("utf-8")

    # Split into ~500 character chunks
    doc_chunks = [text[i:i+500] for i in range(0, len(text), 500)]

    # Create TF-IDF embeddings
    vectorizer = TfidfVectorizer().fit_transform(doc_chunks)
    embeddings = vectorizer.toarray()

    return f"‚úÖ Document processed with {len(doc_chunks)} chunks."

def search_doc(query, top_k=3):
    """Retrieve top-k relevant doc chunks"""
    if not doc_chunks:
        return []
    query_vec = TfidfVectorizer().fit(doc_chunks).transform([query]).toarray()
    sims = cosine_similarity(query_vec, embeddings)[0]
    top_indices = sims.argsort()[-top_k:][::-1]
    return [doc_chunks[i] for i in top_indices]

# =========================
# üîç Web Search
# =========================
def search_web(query):
    url = "https://serpapi.com/search"
    params = {"q": query, "api_key": SERPAPI_KEY}
    res = requests.get(url, params=params)
    if res.status_code == 200:
        data = res.json()
        return (
            data.get("answer_box", {}).get("answer")
            or data.get("organic_results", [{}])[0].get("snippet", "No results found.")
        )
    return "Search failed."

# =========================
# ü§ñ Groq LLM Call
# =========================
def ask_groq(query, doc_context="", web_context=""):
    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"}
    payload = {
        "model": "llama-3.3-70b-versatile",  # ‚úÖ supported model
        "messages": [
            {"role": "system", "content": "Always respond in JSON with keys: answer, context, confidence, sources."},
            {"role": "user", "content": f"Document context: {doc_context}\n\nWeb context: {web_context}\n\nQuestion: {query}"}
        ],
        "temperature": 0.3
    }
    res = requests.post(url, headers=headers, json=payload)
    res.raise_for_status()
    return res.json()["choices"][0]["message"]["content"]

# =========================
# üí¨ Chatbot Logic
# =========================
def chatbot(message, history):
    # RAG doc retrieval
    doc_context = " ".join(search_doc(message, top_k=3))
    # Web search retrieval
    web_context = search_web(message)
    # Call Groq
    response_text = ask_groq(message, doc_context, web_context)

    # Ensure valid JSON
    try:
        parsed = json.loads(response_text)
    except:
        parsed = {
            "answer": response_text,
            "context": doc_context[:200],
            "confidence": "N/A",
            "sources": []
        }

    # Pretty-print JSON
    formatted_json = json.dumps(parsed, indent=2, ensure_ascii=False)

    # Simulate streaming (async effect)
    streamed = ""
    for line in formatted_json.splitlines():
        streamed += line + "\n"
        yield streamed
        time.sleep(0.1)

# =========================
# üé® Gradio UI
# =========================
with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("## üìë RAG + Web AI Agent (Groq + SERPAPI)")

    with gr.Row():
        file_upload = gr.File(label="Upload Document", type="filepath")
        upload_btn = gr.Button("Process Document")
        status = gr.Textbox(label="Status", interactive=False)
    upload_btn.click(fn=process_document, inputs=file_upload, outputs=status)

    gr.ChatInterface(fn=chatbot, type="messages")

demo.launch(share=True)